
# ğŸ› ï¸ Sound-Based Drone Fault Classification using Multi-Microphone Data

This repository contains a complete pipeline for **multi-label audio classification** of drone flight data using **deep learning**. The system processes `.wav` audio files from multiple microphones to predict:

- âœ… **Maneuvering direction** (e.g., left, right, up)
- âš™ï¸ **Fault status** (e.g., normal, propeller fault, motor fault)
- âœˆï¸ **Drone model type**

Audio features are extracted using MFCC, Chroma, and Mel spectrograms.

---

## ğŸ“‚ Dataset

Dataset used:  
[Kaggle: Sound-Based Drone Fault Classification](https://www.kaggle.com/competitions/sound-based-drone-fault-classification-using-multi)

### Directory Structure:
```
/Dataset
    /drone_A
        /A
            /train
                /mic1
                /mic2
            /test
                /mic1
                /mic2
    /drone_B ...
    /drone_C ...
```

### File Naming Convention:
- **Model type:** character `0` (e.g., `"A"`)
- **Maneuvering direction:** character `2`
- **Fault type:** characters `4â€“7`

---

## ğŸš€ Project Pipeline

### ğŸ”Š 1. Feature Extraction
Each audio file is processed with `librosa`:
- **MFCCs**
- **Chroma STFT**
- **Mel Spectrogram**

Mean values of each feature are concatenated to form the final vector.

---

### ğŸ§  2. Multi-Output CNN Model

A **shared Conv1D CNN** backbone feeds into 3 separate dense output heads:
- **Main Output:** Maneuvering Direction
- **Auxiliary Output 1:** Fault Type
- **Auxiliary Output 2:** Drone Model Type

All heads use `softmax` with `categorical_crossentropy`.

---

### ğŸ“Š 3. Training & Evaluation

- **Input:** Combined features from all drones/mics
- **Labels:** One-hot encoded (via `LabelEncoder + to_categorical`)
- **Training:** `Adam` optimizer, 20 epochs
- **Evaluation:** Accuracy reported for each of the 3 outputs

---

### ğŸ§¾ 4. Submission Format

Predictions are saved to `submission1.csv`:

| ID            | model_type | maneuvering_direction | fault |
|---------------|------------|------------------------|-------|
| A_L_N.wav     | A          | L                      | N     |

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install librosa tensorflow scikit-learn pandas
```

---

## â–¶ï¸ Running the Code

1. Make sure the dataset is available under the correct folder structure.
2. Run the script:  
   `audio_classification.py` (in Kaggle, Jupyter, or locally)
3. Check for:
   - Training metrics
   - Test accuracies
   - `submission1.csv` generated

---

## ğŸ§  Model Architecture Summary

```
Input â†’ Conv1D + Pool â†’ Conv1D + Pool â†’ Conv1D + Pool â†’ Flatten
   â”œâ”€â”€â†’ Dense â†’ Dropout â†’ Output: Maneuver Direction
   â”œâ”€â”€â†’ Dense â†’ Dropout â†’ Output: Fault Type
   â””â”€â”€â†’ Dense â†’ Dropout â†’ Output: Model Type
```

---

## ğŸ“ˆ Results (Sample)

| Task                    | Accuracy (Test Set) |
|-------------------------|---------------------|
| Maneuver Direction      | XX.XX%              |
| Fault Classification    | XX.XX%              |
| Model Type Classification | XX.XX%           |

> Replace `XX.XX%` after training the model.

---

## ğŸ“ Folder Structure

```
audio_classification/
â”œâ”€â”€ audio_classification.py
â”œâ”€â”€ README.md
â”œâ”€â”€ submission1.csv
â””â”€â”€ Dataset/  # (not included due to size)
```

---

## âœï¸ Author

**Swaroop Itkikar**  
Learning enthusiast, passionate about AI, Audio Processing, and Deep Learning.

---

## ğŸ“Œ Notes

- You can further improve accuracy using spectrogram images + CNNs
- Data augmentation or ensemble models may help
- Pretrained audio models (like VGGish, OpenL3) can also be explored
